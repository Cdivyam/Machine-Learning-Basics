# -*- coding: utf-8 -*-
"""Linear Regression Math.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pbqUbX88kCfswxz6Tf1_BFiDSHlfJ-vd
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline

# define data size
n = 300

np.random.seed(0)           # random seed for constant output
X = np.linspace(0, 1, n)    # have linearly spaced 'n' numbers in X
X = X.reshape(n,1)          # reshape from unranked array to ranked array
X.shape

Y = 20*X + 7 + np.random.normal(0, 0.8, X.shape)  # have linearly spaced numbers in Y with gausian noise
Y.shape

Y[:5]

sns.set()
sns.scatterplot(x = X[:, 0], y = Y[:, 0])         #Plot current dataset
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

# shuffle and split data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 5)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# function to initialize parameters
def initialize_parameters(dim):
  w1 = np.random.rand()
  w0 = np.random.rand()

  return w1, w0

# function to propagate through the Machine Learning Model
def propagate(w0, w1, X, Y):
  m = Y.shape[0]
  
  y_hat = np.multiply(X, w1) + w0 # hypothesis function
  
  J = 0.5/m*(np.sum(np.power((Y - y_hat), 2))) # cost function
  
  dw1 = 1/m * (np.sum(np.multiply((y_hat - Y), X))) # gradient descent for w1 
  dw0 = 1/m * np.sum(y_hat - Y) # gradient descent for w0
  
  grads = { 'dw1' : dw1,  # keep gradients at one place - dw0, dw1
            'dw0' : dw0}
  
  return grads, J

# function to optimize parameters
def optimize(w0, w1, X, Y, num_iter, learn_rate):
  
  costs = [] # list for all the costs
  for i in range(num_iter):
    grads, cost = propagate(w0, w1, X, Y) # propagate through the linear regression
    
    dw1 = grads['dw1']
    dw0 = grads['dw0']
    
    # optimize the gradients
    w1 = w1 - learn_rate*dw1 
    w0 = w0 - learn_rate*dw0 
    
    if i%1000 == 0:
      print(cost)
      costs.append(cost)
  params = {'w1' : w1, 'w0' : w0}
  
  grads = { 'dw1' : dw1, 'dw0' : dw0}
  
  return params, grads, costs

# predict the test dataset
def predict(X, w1, w0):

  Y = X*w1 + w0

  return Y

def model(X_train, y_train, X_test, y_test, num_iter, learn_rate):
  
  # initialize parameters
  w1, w0 = initialize_parameters(X_train.shape[0])
  
  # propagate and optimize
  params, grads, costs = optimize(w0, w1, X_train, y_train, num_iter, learn_rate)
  
  w1 = params['w1']
  w0 = params['w0']

  # predict test set
  y_preds = predict(X_test, w1, w0)
  
  return w1, w0, costs, y_preds

w1, w0, costs, y_preds = model(X_train, y_train, X_test, y_test, num_iter = 50000, learn_rate = 0.001)

# plot the costs
sns.set(style= 'darkgrid')
sns.lineplot(x = range(len(costs)), y = costs)

# print the weights w0 and w1
print(w1)
print(w0)

# plot the regression model
sns.set()
sns.scatterplot(x = X_test[:, 0], y = y_test[:, 0])
plt.plot(X_test, y_preds, color = 'r')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

#Rsquared Error
def r_squared(X, Y):

  n = X.shape[0]
  sX = np.sum(X)
  sY = np.sum(Y)
  sXY = np.sum(np.multiply(X, Y))
  sX2 = np.sum(np.square(X))
  sY2 = np.sum(np.square(Y))
  numerator = n*sXY - sX*sY

  den = (n*sX2-sX**2)*(n*sY2-sY**2)
  denominator = np.sqrt(den)

  rsqr = numerator/denominator

  return rsqr

train_score = r_squared(X_train, y_train)
print(train_score)

# 98 % of outputs(Y_train) can be explained by our inputs(X_train)
# similarly you can find test_score as well
